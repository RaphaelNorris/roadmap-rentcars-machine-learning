{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - CRISP-DM Template\n",
    "# {{ cookiecutter.project_name }}\n",
    "\n",
    "**CRISP-DM Phases 1-2: Business Understanding & Data Understanding**\n",
    "\n",
    "---\n",
    "\n",
    "## Document Control\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Project** | {{ cookiecutter.project_name }} |\n",
    "| **Author** | {{ cookiecutter.author_name }} |\n",
    "| **Contact** | {{ cookiecutter.email }} |\n",
    "| **Date** | YYYY-MM-DD |\n",
    "| **Version** | 1.0 |\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Business Understanding](#1-business-understanding)\n",
    "2. [Data Understanding](#2-data-understanding)\n",
    "3. [Data Quality Assessment](#3-data-quality-assessment)\n",
    "4. [Exploratory Analysis](#4-exploratory-analysis)\n",
    "5. [Statistical Analysis](#5-statistical-analysis)\n",
    "6. [Feature Analysis](#6-feature-analysis)\n",
    "7. [Hypothesis Testing](#7-hypothesis-testing)\n",
    "8. [Initial Insights](#8-initial-insights)\n",
    "9. [Next Steps](#9-next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, mannwhitneyu, ks_2samp\n",
    "\n",
    "# Data profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Data validation\n",
    "import pandera as pa\n",
    "from pandera import Column, DataFrameSchema\n",
    "\n",
    "# Project modules\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "from src.data.aws_integration import get_s3_client, get_athena_client, get_iceberg_manager\n",
    "from src.utils.logger import get_logger\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "print(\"Setup completo!\")\n",
    "print(f\"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Business Understanding\n",
    "\n",
    "**CRISP-DM Phase 1**: Entender os objetivos e requisitos de neg√≥cio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Business Objectives\n",
    "\n",
    "**Problema de Neg√≥cio**:\n",
    "- [Descreva o problema de neg√≥cio que estamos tentando resolver]\n",
    "- [Exemplo: Reduzir churn de clientes em 20%]\n",
    "\n",
    "**Objetivos SMART**:\n",
    "1. **Specific**: [Objetivo espec√≠fico]\n",
    "2. **Measurable**: [Como ser√° medido - KPI]\n",
    "3. **Achievable**: [√â alcan√ß√°vel? Baseado em qu√™?]\n",
    "4. **Relevant**: [Por que √© relevante para o neg√≥cio?]\n",
    "5. **Time-bound**: [Prazo - quando deve ser alcan√ßado?]\n",
    "\n",
    "**Success Criteria**:\n",
    "- M√©trica de Neg√≥cio: [Ex: Redu√ß√£o de 20% no churn]\n",
    "- M√©trica de ML: [Ex: Precision > 0.80, Recall > 0.70]\n",
    "- ROI Esperado: [Ex: R$ X milh√µes/ano]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Business Questions\n",
    "\n",
    "**Perguntas principais que queremos responder**:\n",
    "\n",
    "1. [Pergunta 1: Ex: Quais caracter√≠sticas mais diferenciam clientes que churn vs n√£o-churn?]\n",
    "2. [Pergunta 2: Ex: Existe sazonalidade no churn?]\n",
    "3. [Pergunta 3: Ex: Qual o perfil de cliente com maior risco?]\n",
    "4. [Pergunta 4: Ex: Quanto tempo antes podemos prever o churn?]\n",
    "5. [Pergunta 5: Ex: Quais a√ß√µes de reten√ß√£o s√£o mais efetivas?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ML Problem Definition\n",
    "\n",
    "**Tradu√ß√£o para problema de ML**:\n",
    "\n",
    "- **Problem Type**: [Classification / Regression / Clustering / Ranking]\n",
    "- **Target Variable**: [Nome da vari√°vel alvo e defini√ß√£o]\n",
    "- **Prediction Horizon**: [Ex: Prever churn nos pr√≥ximos 30 dias]\n",
    "- **Features**: [Tipos de features que usaremos - demogr√°ficas, comportamentais, transacionais]\n",
    "- **Model Constraints**: [Lat√™ncia < 100ms, Explicabilidade necess√°ria, etc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Understanding\n",
    "\n",
    "**CRISP-DM Phase 2**: Coletar, descrever e explorar os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o de acesso aos dados\n",
    "DATA_SOURCE = \"s3\"  # s3 / athena / iceberg / local\n",
    "S3_PATH = \"s3://bucket-name/path/to/data.parquet\"\n",
    "ATHENA_QUERY = \"SELECT * FROM database.table WHERE date >= '2024-01-01'\"\n",
    "LOCAL_PATH = \"data/raw/sample_data.csv\"\n",
    "\n",
    "# Per√≠odo de an√°lise\n",
    "START_DATE = \"2024-01-01\"\n",
    "END_DATE = \"2024-12-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "logger.info(f\"Carregando dados de {DATA_SOURCE}...\")\n",
    "\n",
    "if DATA_SOURCE == \"s3\":\n",
    "    s3_client = get_s3_client()\n",
    "    df = s3_client.read_parquet(s3_key=S3_PATH.replace(\"s3://\", \"\").split(\"/\", 1)[1])\n",
    "    \n",
    "elif DATA_SOURCE == \"athena\":\n",
    "    athena_client = get_athena_client()\n",
    "    df = athena_client.execute_query(ATHENA_QUERY)\n",
    "    \n",
    "elif DATA_SOURCE == \"iceberg\":\n",
    "    iceberg = get_iceberg_manager()\n",
    "    df = iceberg.read_table(table_name=\"feature_store\")\n",
    "    \n",
    "else:  # local\n",
    "    df = pd.read_csv(LOCAL_PATH)\n",
    "\n",
    "logger.info(f\"Dados carregados: {df.shape[0]:,} linhas, {df.shape[1]} colunas\")\n",
    "print(f\"\\nüìä Dataset carregado com sucesso!\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview geral\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiras linhas\n",
    "print(\"\\nPrimeiras 5 linhas:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas descritivas\n",
    "print(\"\\nEstat√≠sticas Descritivas (Features Num√©ricas):\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas descritivas (categ√≥ricas)\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"\\nEstat√≠sticas Descritivas (Features Categ√≥ricas):\")\n",
    "    display(df[categorical_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Dictionary\n",
    "\n",
    "**Documentar cada feature**:\n",
    "\n",
    "| Feature | Type | Description | Business Meaning | Source |\n",
    "|---------|------|-------------|------------------|--------|\n",
    "| feature_1 | int | [Descri√ß√£o t√©cnica] | [O que significa para o neg√≥cio] | [Tabela origem] |\n",
    "| feature_2 | float | ... | ... | ... |\n",
    "| target | binary | ... | ... | ... |\n",
    "\n",
    "*Preencha esta tabela com todas as features do dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Automated Data Profiling\n",
    "\n",
    "**Gerar relat√≥rio autom√°tico com ydata-profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar profile report (pode demorar para datasets grandes)\n",
    "# Descomente para executar\n",
    "\n",
    "# profile = ProfileReport(\n",
    "#     df,\n",
    "#     title=\"{{ cookiecutter.project_name }} - Data Profiling Report\",\n",
    "#     explorative=True,\n",
    "#     correlations={\n",
    "#         \"pearson\": {\"calculate\": True},\n",
    "#         \"spearman\": {\"calculate\": True},\n",
    "#         \"kendall\": {\"calculate\": False},\n",
    "#         \"phi_k\": {\"calculate\": False},\n",
    "#     },\n",
    "#     missing_diagrams={\n",
    "#         \"matrix\": True,\n",
    "#         \"bar\": True,\n",
    "#         \"heatmap\": True,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # Salvar relat√≥rio\n",
    "# output_path = \"reports/data_profiling_report.html\"\n",
    "# profile.to_file(output_path)\n",
    "# print(f\"Relat√≥rio salvo em: {output_path}\")\n",
    "\n",
    "# # Exibir no notebook\n",
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de valores faltantes\n",
    "missing_stats = pd.DataFrame({\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percent': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    'Dtype': df.dtypes,\n",
    "    'Unique_Values': df.nunique(),\n",
    "    'Sample_Values': [df[col].dropna().head(3).tolist() for col in df.columns]\n",
    "})\n",
    "\n",
    "missing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values(\n",
    "    'Missing_Percent', ascending=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "if len(missing_stats) > 0:\n",
    "    display(missing_stats)\n",
    "else:\n",
    "    print(\"‚úì Nenhum valor faltante encontrado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar padr√£o de missing values\n",
    "if len(missing_stats) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Bar plot\n",
    "    missing_stats.sort_values('Missing_Percent')['Missing_Percent'].plot(\n",
    "        kind='barh', ax=axes[0], color='coral'\n",
    "    )\n",
    "    axes[0].set_xlabel('Missing Percentage')\n",
    "    axes[0].set_title('Missing Values by Feature')\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Heatmap (sample)\n",
    "    missing_cols = missing_stats.head(20).index.tolist()\n",
    "    sns.heatmap(\n",
    "        df[missing_cols].isnull().head(100),\n",
    "        cbar=False,\n",
    "        yticklabels=False,\n",
    "        cmap='RdYlGn_r',\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title('Missing Values Pattern (first 100 rows)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Duplicate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar duplicatas\n",
    "n_duplicates = df.duplicated().sum()\n",
    "duplicate_pct = (n_duplicates / len(df) * 100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DUPLICATE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total duplicates: {n_duplicates:,} ({duplicate_pct:.2f}%)\")\n",
    "\n",
    "if n_duplicates > 0:\n",
    "    print(\"\\nExemplo de duplicatas:\")\n",
    "    display(df[df.duplicated(keep=False)].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Types Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar tipos de dados\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA TYPES DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "print(\"\\nContagem por tipo:\")\n",
    "print(dtype_counts)\n",
    "\n",
    "# Listar por categoria\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric columns ({len(numeric_cols)}): {numeric_cols[:5]}...\")\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols[:5]}...\")\n",
    "print(f\"Datetime columns ({len(datetime_cols)}): {datetime_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Schema Validation\n",
    "\n",
    "**Valida√ß√£o usando Pandera**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir schema esperado (exemplo)\n",
    "# Customize de acordo com suas features\n",
    "\n",
    "# schema = DataFrameSchema({\n",
    "#     \"customer_id\": Column(str, nullable=False, unique=True),\n",
    "#     \"age\": Column(int, checks=pa.Check.in_range(min_value=18, max_value=120)),\n",
    "#     \"income\": Column(float, checks=pa.Check.greater_than(0)),\n",
    "#     \"target\": Column(int, checks=pa.Check.isin([0, 1])),\n",
    "# })\n",
    "\n",
    "# # Validar\n",
    "# try:\n",
    "#     validated_df = schema.validate(df)\n",
    "#     print(\"‚úì Schema validation passed!\")\n",
    "# except pa.errors.SchemaError as e:\n",
    "#     print(\"‚úó Schema validation failed!\")\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo de qualidade\n",
    "quality_metrics = {\n",
    "    'Total Records': len(df),\n",
    "    'Total Features': len(df.columns),\n",
    "    'Completeness': f\"{((1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100):.2f}%\",\n",
    "    'Duplicates': f\"{duplicate_pct:.2f}%\",\n",
    "    'Memory Usage': f\"{df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\",\n",
    "    'Numeric Features': len(numeric_cols),\n",
    "    'Categorical Features': len(categorical_cols),\n",
    "    'Datetime Features': len(datetime_cols),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for metric, value in quality_metrics.items():\n",
    "    print(f\"{metric:.<30} {value:>20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina sua vari√°vel target\n",
    "TARGET_COL = \"target\"  # Altere para sua vari√°vel target\n",
    "\n",
    "if TARGET_COL in df.columns:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TARGET VARIABLE ANALYSIS: {TARGET_COL}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Distribui√ß√£o\n",
    "    target_dist = df[TARGET_COL].value_counts()\n",
    "    target_pct = df[TARGET_COL].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(\"\\nDistribui√ß√£o:\")\n",
    "    for val in target_dist.index:\n",
    "        print(f\"  {val}: {target_dist[val]:,} ({target_pct[val]:.2f}%)\")\n",
    "    \n",
    "    # Visualizar\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Count plot\n",
    "    target_dist.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "    axes[0].set_title(f'Target Distribution: {TARGET_COL}')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_xlabel('')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(\n",
    "        target_dist.values,\n",
    "        labels=target_dist.index,\n",
    "        autopct='%1.1f%%',\n",
    "        colors=['#2ecc71', '#e74c3c'],\n",
    "        startangle=90\n",
    "    )\n",
    "    axes[1].set_title('Target Proportion')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Verificar desbalanceamento\n",
    "    imbalance_ratio = target_dist.max() / target_dist.min()\n",
    "    print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "    if imbalance_ratio > 3:\n",
    "        print(\"‚ö†Ô∏è  Dataset desbalanceado! Considere t√©cnicas de resampling.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Coluna '{TARGET_COL}' n√£o encontrada no dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Numeric Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribui√ß√µes de features num√©ricas\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NUMERIC FEATURES DISTRIBUTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Selecionar top N features para visualizar\n",
    "    n_features = min(12, len(numeric_cols))\n",
    "    features_to_plot = numeric_cols[:n_features]\n",
    "    \n",
    "    # Histogramas\n",
    "    n_cols = 4\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(features_to_plot):\n",
    "        df[col].hist(bins=50, ax=axes[idx], edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f'{col}\\nMean: {df[col].mean():.2f}, Std: {df[col].std():.2f}')\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "    \n",
    "    # Esconder subplots vazios\n",
    "    for idx in range(n_features, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots para detectar outliers\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\nOutlier Detection (Box Plots):\")\n",
    "    \n",
    "    n_features = min(12, len(numeric_cols))\n",
    "    features_to_plot = numeric_cols[:n_features]\n",
    "    \n",
    "    n_cols = 4\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(features_to_plot):\n",
    "        df.boxplot(column=col, ax=axes[idx])\n",
    "        axes[idx].set_title(col)\n",
    "        \n",
    "        # Calcular outliers\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "        outlier_pct = (outliers / len(df) * 100)\n",
    "        axes[idx].set_xlabel(f'Outliers: {outliers} ({outlier_pct:.1f}%)')\n",
    "    \n",
    "    for idx in range(n_features, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de features categ√≥ricas\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CATEGORICAL FEATURES ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for col in categorical_cols[:10]:  # Top 10\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Unique values: {df[col].nunique()}\")\n",
    "        print(f\"  Top 5 values:\")\n",
    "        value_counts = df[col].value_counts().head(5)\n",
    "        for val, count in value_counts.items():\n",
    "            pct = (count / len(df) * 100)\n",
    "            print(f\"    {val}: {count:,} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar categ√≥ricas\n",
    "if len(categorical_cols) > 0:\n",
    "    # Selecionar features com cardinalidade razo√°vel\n",
    "    plottable_cats = [col for col in categorical_cols if df[col].nunique() <= 20]\n",
    "    n_features = min(8, len(plottable_cats))\n",
    "    features_to_plot = plottable_cats[:n_features]\n",
    "    \n",
    "    if len(features_to_plot) > 0:\n",
    "        n_cols = 2\n",
    "        n_rows = (n_features + n_cols - 1) // n_cols\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 4))\n",
    "        axes = axes.flatten() if n_features > 1 else [axes]\n",
    "        \n",
    "        for idx, col in enumerate(features_to_plot):\n",
    "            df[col].value_counts().head(10).plot(kind='barh', ax=axes[idx])\n",
    "            axes[idx].set_title(f'{col} (Top 10)')\n",
    "            axes[idx].set_xlabel('Count')\n",
    "        \n",
    "        for idx in range(n_features, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Temporal Analysis\n",
    "\n",
    "*Se houver features temporais*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise temporal (se aplic√°vel)\n",
    "if len(datetime_cols) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEMPORAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for date_col in datetime_cols:\n",
    "        print(f\"\\n{date_col}:\")\n",
    "        print(f\"  Min date: {df[date_col].min()}\")\n",
    "        print(f\"  Max date: {df[date_col].max()}\")\n",
    "        print(f\"  Date range: {(df[date_col].max() - df[date_col].min()).days} days\")\n",
    "        \n",
    "        # Time series plot\n",
    "        fig, ax = plt.subplots(figsize=(14, 5))\n",
    "        df.groupby(df[date_col].dt.to_period('D')).size().plot(ax=ax)\n",
    "        ax.set_title(f'Records Over Time: {date_col}')\n",
    "        ax.set_ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correla√ß√£o\n",
    "if len(numeric_cols) > 1:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calcular correla√ß√£o\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        mask=mask,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=1,\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    plt.title('Correlation Matrix (Numeric Features)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Top correla√ß√µes\n",
    "    print(\"\\nTop 10 correla√ß√µes mais fortes (|r| > 0.5):\")\n",
    "    corr_pairs = corr_matrix.unstack()\n",
    "    corr_pairs = corr_pairs[corr_pairs != 1.0]  # Remove self-correlation\n",
    "    corr_pairs = corr_pairs.sort_values(ascending=False)\n",
    "    corr_pairs = corr_pairs[abs(corr_pairs) > 0.5].head(10)\n",
    "    \n",
    "    for (feat1, feat2), corr_val in corr_pairs.items():\n",
    "        print(f\"  {feat1} <-> {feat2}: r = {corr_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Importance (Target Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correla√ß√£o com target\n",
    "if TARGET_COL in df.columns and df[TARGET_COL].dtype in [np.int64, np.float64]:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE IMPORTANCE (Correlation with Target)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Correla√ß√£o de features num√©ricas com target\n",
    "    target_corr = df[numeric_cols + [TARGET_COL]].corr()[TARGET_COL].sort_values(ascending=False)\n",
    "    target_corr = target_corr.drop(TARGET_COL)  # Remove target self-correlation\n",
    "    \n",
    "    print(\"\\nTop 15 features correlacionadas com target:\")\n",
    "    print(target_corr.head(15))\n",
    "    \n",
    "    # Visualizar\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    target_corr.head(20).plot(kind='barh', ax=ax, color='steelblue')\n",
    "    ax.set_title(f'Top 20 Features Correlated with {TARGET_COL}')\n",
    "    ax.set_xlabel('Correlation Coefficient')\n",
    "    ax.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Normality Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar normalidade (Shapiro-Wilk para amostras pequenas, Kolmogorov-Smirnov para grandes)\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NORMALITY TESTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    normality_results = []\n",
    "    \n",
    "    for col in numeric_cols[:20]:  # Testar top 20\n",
    "        # Remove NaN\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        if len(data) < 5000:\n",
    "            stat, p_value = stats.shapiro(data.sample(min(5000, len(data))))\n",
    "            test_name = \"Shapiro-Wilk\"\n",
    "        else:\n",
    "            stat, p_value = stats.kstest(data, 'norm')\n",
    "            test_name = \"Kolmogorov-Smirnov\"\n",
    "        \n",
    "        is_normal = \"Yes\" if p_value > 0.05 else \"No\"\n",
    "        normality_results.append({\n",
    "            'Feature': col,\n",
    "            'Test': test_name,\n",
    "            'Statistic': stat,\n",
    "            'P-value': p_value,\n",
    "            'Normal (Œ±=0.05)': is_normal\n",
    "        })\n",
    "    \n",
    "    normality_df = pd.DataFrame(normality_results)\n",
    "    display(normality_df.sort_values('P-value', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Univariate Analysis by Target\n",
    "\n",
    "*Analisar como cada feature se comporta nos diferentes valores de target*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise univariada por target (features num√©ricas)\n",
    "if TARGET_COL in df.columns and len(numeric_cols) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"UNIVARIATE ANALYSIS BY TARGET (Numeric)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Selecionar top features\n",
    "    n_features = min(8, len(numeric_cols))\n",
    "    features_to_plot = numeric_cols[:n_features]\n",
    "    \n",
    "    n_cols = 2\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 5))\n",
    "    axes = axes.flatten() if n_features > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(features_to_plot):\n",
    "        # Box plot por target\n",
    "        df.boxplot(column=col, by=TARGET_COL, ax=axes[idx])\n",
    "        axes[idx].set_title(f'{col} by {TARGET_COL}')\n",
    "        axes[idx].set_xlabel(TARGET_COL)\n",
    "        \n",
    "        # Adicionar m√©dia\n",
    "        means = df.groupby(TARGET_COL)[col].mean()\n",
    "        for target_val, mean_val in means.items():\n",
    "            axes[idx].text(\n",
    "                target_val + 1, mean_val,\n",
    "                f'Œº={mean_val:.2f}',\n",
    "                ha='center', va='bottom'\n",
    "            )\n",
    "    \n",
    "    for idx in range(n_features, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('')  # Remove auto title\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise univariada por target (features categ√≥ricas)\n",
    "if TARGET_COL in df.columns and len(categorical_cols) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"UNIVARIATE ANALYSIS BY TARGET (Categorical)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Selecionar categ√≥ricas com baixa cardinalidade\n",
    "    plottable_cats = [col for col in categorical_cols if df[col].nunique() <= 10]\n",
    "    n_features = min(6, len(plottable_cats))\n",
    "    features_to_plot = plottable_cats[:n_features]\n",
    "    \n",
    "    if len(features_to_plot) > 0:\n",
    "        n_cols = 2\n",
    "        n_rows = (n_features + n_cols - 1) // n_cols\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 5))\n",
    "        axes = axes.flatten() if n_features > 1 else [axes]\n",
    "        \n",
    "        for idx, col in enumerate(features_to_plot):\n",
    "            # Crosstab\n",
    "            ct = pd.crosstab(df[col], df[TARGET_COL], normalize='index') * 100\n",
    "            ct.plot(kind='bar', ax=axes[idx], stacked=False)\n",
    "            axes[idx].set_title(f'{col} vs {TARGET_COL}')\n",
    "            axes[idx].set_ylabel('Percentage')\n",
    "            axes[idx].legend(title=TARGET_COL)\n",
    "            axes[idx].set_xlabel('')\n",
    "        \n",
    "        for idx in range(n_features, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots bivariados (top features vs target)\n",
    "if TARGET_COL in df.columns and len(numeric_cols) >= 2:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BIVARIATE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Selecionar top 6 features mais correlacionadas com target\n",
    "    if df[TARGET_COL].dtype in [np.int64, np.float64]:\n",
    "        top_features = df[numeric_cols + [TARGET_COL]].corr()[TARGET_COL].abs().sort_values(ascending=False)\n",
    "        top_features = top_features.drop(TARGET_COL).head(6).index.tolist()\n",
    "    else:\n",
    "        top_features = numeric_cols[:6]\n",
    "    \n",
    "    if len(top_features) >= 2:\n",
    "        # Criar pairplot\n",
    "        sample_df = df[top_features + [TARGET_COL]].sample(\n",
    "            min(5000, len(df)), random_state=42\n",
    "        )\n",
    "        \n",
    "        pairplot = sns.pairplot(\n",
    "            sample_df,\n",
    "            hue=TARGET_COL,\n",
    "            diag_kind='kde',\n",
    "            plot_kws={'alpha': 0.6},\n",
    "            height=3\n",
    "        )\n",
    "        pairplot.fig.suptitle('Pairplot - Top Features vs Target', y=1.01)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Feature Engineering Ideas\n",
    "\n",
    "*Documentar ideias para feature engineering baseadas na EDA*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ideias de Feature Engineering** (baseadas na an√°lise acima):\n",
    "\n",
    "1. **Interaction Features**:\n",
    "   - [Ex: feature_1 * feature_2 - se mostraram correla√ß√£o interessante]\n",
    "   - [Ex: ratio de feature_3 / feature_4]\n",
    "\n",
    "2. **Aggregations**:\n",
    "   - [Ex: M√©dia de transa√ß√µes por cliente]\n",
    "   - [Ex: Total de compras nos √∫ltimos 30/60/90 dias]\n",
    "\n",
    "3. **Binning/Discretization**:\n",
    "   - [Ex: Idade em faixas: 18-25, 26-35, etc]\n",
    "   - [Ex: Income em quartis]\n",
    "\n",
    "4. **Temporal Features**:\n",
    "   - [Ex: Day of week, month, quarter]\n",
    "   - [Ex: Days since last purchase]\n",
    "   - [Ex: Recency, Frequency, Monetary (RFM)]\n",
    "\n",
    "5. **Encoding Strategies**:\n",
    "   - [Ex: Target encoding para categ√≥ricas de alta cardinalidade]\n",
    "   - [Ex: One-hot encoding para baixa cardinalidade]\n",
    "\n",
    "6. **Missing Value Indicators**:\n",
    "   - [Ex: is_missing_feature_X (binary flag)]\n",
    "\n",
    "*Documente aqui suas ideias espec√≠ficas baseadas nos dados*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testes estat√≠sticos para validar hip√≥teses\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Exemplo: Testar se features num√©ricas diferem entre classes do target\n",
    "if TARGET_COL in df.columns and len(numeric_cols) > 0:\n",
    "    target_classes = df[TARGET_COL].unique()\n",
    "    \n",
    "    if len(target_classes) == 2:\n",
    "        print(\"\\nMann-Whitney U Test (diferen√ßas entre grupos):\")\n",
    "        print(\"H0: Distribui√ß√µes s√£o iguais entre classes\")\n",
    "        print(\"H1: Distribui√ß√µes s√£o diferentes\\n\")\n",
    "        \n",
    "        test_results = []\n",
    "        \n",
    "        for col in numeric_cols[:10]:  # Top 10\n",
    "            group1 = df[df[TARGET_COL] == target_classes[0]][col].dropna()\n",
    "            group2 = df[df[TARGET_COL] == target_classes[1]][col].dropna()\n",
    "            \n",
    "            if len(group1) > 0 and len(group2) > 0:\n",
    "                stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "                \n",
    "                test_results.append({\n",
    "                    'Feature': col,\n",
    "                    'Statistic': stat,\n",
    "                    'P-value': p_value,\n",
    "                    'Significant (Œ±=0.05)': 'Yes' if p_value < 0.05 else 'No',\n",
    "                    'Mean Group 0': group1.mean(),\n",
    "                    'Mean Group 1': group2.mean(),\n",
    "                })\n",
    "        \n",
    "        results_df = pd.DataFrame(test_results)\n",
    "        display(results_df.sort_values('P-value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test para features categ√≥ricas vs target\n",
    "if TARGET_COL in df.columns and len(categorical_cols) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Chi-Square Test (Categorical Features vs Target)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"H0: Features s√£o independentes do target\")\n",
    "    print(\"H1: Features s√£o dependentes do target\\n\")\n",
    "    \n",
    "    chi2_results = []\n",
    "    \n",
    "    for col in categorical_cols[:10]:\n",
    "        if df[col].nunique() < 50:  # Evitar alta cardinalidade\n",
    "            contingency_table = pd.crosstab(df[col], df[TARGET_COL])\n",
    "            chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "            \n",
    "            chi2_results.append({\n",
    "                'Feature': col,\n",
    "                'Chi2': chi2,\n",
    "                'P-value': p_value,\n",
    "                'DOF': dof,\n",
    "                'Significant (Œ±=0.05)': 'Yes' if p_value < 0.05 else 'No'\n",
    "            })\n",
    "    \n",
    "    if len(chi2_results) > 0:\n",
    "        chi2_df = pd.DataFrame(chi2_results)\n",
    "        display(chi2_df.sort_values('P-value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Business Hypotheses\n",
    "\n",
    "*Testar hip√≥teses espec√≠ficas do neg√≥cio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hip√≥teses de Neg√≥cio para Testar**:\n",
    "\n",
    "1. **H1**: [Exemplo: Clientes premium t√™m menor taxa de churn que clientes regulares]\n",
    "   - Teste: [Mann-Whitney / Chi-square]\n",
    "   - Resultado: [Aceitar/Rejeitar H0]\n",
    "   - Implica√ß√£o: [O que isso significa para o neg√≥cio]\n",
    "\n",
    "2. **H2**: [Exemplo: Clientes que usaram suporte nos √∫ltimos 30 dias t√™m maior churn]\n",
    "   - Teste: [...]\n",
    "   - Resultado: [...]\n",
    "   - Implica√ß√£o: [...]\n",
    "\n",
    "3. **H3**: [Sua hip√≥tese]\n",
    "   - ...\n",
    "\n",
    "*Adicione c√≥digo cells abaixo para testar cada hip√≥tese*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar H1\n",
    "# [Seu c√≥digo aqui]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar H2\n",
    "# [Seu c√≥digo aqui]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Initial Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Key Findings\n",
    "\n",
    "**Principais descobertas da EDA**:\n",
    "\n",
    "1. **Data Quality**:\n",
    "   - [Ex: Dataset tem 95% de completeness, apenas 3 features com >10% missing]\n",
    "   - [Ex: 5% de duplicatas identificadas, necess√°rio tratamento]\n",
    "   - [Ex: Outliers significativos em feature X (20% dos dados)]\n",
    "\n",
    "2. **Target Variable**:\n",
    "   - [Ex: Target desbalanceado 80:20, necess√°rio resampling]\n",
    "   - [Ex: Distribui√ß√£o temporal est√°vel, sem mudan√ßas bruscas]\n",
    "\n",
    "3. **Feature Patterns**:\n",
    "   - [Ex: Top 3 features correlacionadas com target: A, B, C]\n",
    "   - [Ex: Feature X tem correla√ß√£o forte (0.7) com target]\n",
    "   - [Ex: Features Y e Z s√£o altamente correlacionadas (0.9), considerar remover uma]\n",
    "\n",
    "4. **Business Insights**:\n",
    "   - [Ex: Clientes premium t√™m 50% menos churn que regulares (p<0.01)]\n",
    "   - [Ex: Sazonalidade identificada: maior churn em dezembro]\n",
    "   - [Ex: Clientes com >5 transa√ß√µes/m√™s raramente churnam]\n",
    "\n",
    "5. **Data Issues**:\n",
    "   - [Ex: Feature idade tem valores negativos - necess√°rio valida√ß√£o]\n",
    "   - [Ex: 30% dos clientes sem informa√ß√£o de segmento]\n",
    "   - [Ex: Dados antes de 2023 parecem incompletos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Recommendations\n",
    "\n",
    "**Recomenda√ß√µes para pr√≥ximas fases**:\n",
    "\n",
    "**Data Preparation (CRISP-DM Phase 3)**:\n",
    "1. [Ex: Tratar missing values com forward fill para features temporais]\n",
    "2. [Ex: Remover duplicatas usando customer_id + date como chave]\n",
    "3. [Ex: Aplicar log transform em features com alta skewness]\n",
    "4. [Ex: Criar feature 'dias_desde_ultima_compra' para capturar rec√™ncia]\n",
    "5. [Ex: Aplicar SMOTE para balancear target]\n",
    "\n",
    "**Modeling (CRISP-DM Phase 4)**:\n",
    "1. [Ex: Testar Random Forest, XGBoost e LightGBM]\n",
    "2. [Ex: Usar time-series split para valida√ß√£o (n√£o random split)]\n",
    "3. [Ex: Feature selection: remover features com correla√ß√£o >0.95]\n",
    "4. [Ex: Otimizar threshold de decis√£o para maximizar recall (catch churners)]\n",
    "\n",
    "**Data Collection**:\n",
    "1. [Ex: Solicitar dados de suporte (tickets, intera√ß√µes)]\n",
    "2. [Ex: Integrar dados de campanhas de marketing]\n",
    "3. [Ex: Adicionar features de produto (uso de features premium)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Action Items\n",
    "\n",
    "**A√ß√µes imediatas**:\n",
    "\n",
    "- [ ] **Data Cleaning**:\n",
    "  - [ ] Tratar missing values\n",
    "  - [ ] Remover duplicatas\n",
    "  - [ ] Validar e corrigir outliers\n",
    "  \n",
    "- [ ] **Feature Engineering**:\n",
    "  - [ ] Criar features temporais (RFM)\n",
    "  - [ ] Criar interaction features\n",
    "  - [ ] Aplicar encoding em categ√≥ricas\n",
    "  \n",
    "- [ ] **Data Pipeline**:\n",
    "  - [ ] Implementar pipeline de transforma√ß√£o em `src/pipelines/DS/feature_pipeline.py`\n",
    "  - [ ] Documentar transforma√ß√µes no Data Card\n",
    "  - [ ] Configurar testes de qualidade de dados\n",
    "  \n",
    "- [ ] **Documentation**:\n",
    "  - [ ] Completar Data Card com estat√≠sticas da EDA\n",
    "  - [ ] Documentar business insights encontrados\n",
    "  - [ ] Atualizar Business Requirements com valida√ß√µes\n",
    "  \n",
    "- [ ] **Modeling**:\n",
    "  - [ ] Preparar dataset limpo para modelagem\n",
    "  - [ ] Definir estrat√©gia de valida√ß√£o (time-series split)\n",
    "  - [ ] Iniciar baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Transition to Next Phase\n",
    "\n",
    "**CRISP-DM Phase 3: Data Preparation**\n",
    "\n",
    "Pr√≥ximo notebook: `02_data_preparation.ipynb`\n",
    "\n",
    "Este notebook deve:\n",
    "1. Implementar todas as transforma√ß√µes identificadas nesta EDA\n",
    "2. Criar vers√£o limpa do dataset\n",
    "3. Salvar em S3/Iceberg para uso em treinamento\n",
    "4. Documentar todas as transforma√ß√µes aplicadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar resumo da EDA para arquivo\n",
    "summary = {\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_shape': df.shape,\n",
    "    'total_features': len(df.columns),\n",
    "    'numeric_features': len(numeric_cols),\n",
    "    'categorical_features': len(categorical_cols),\n",
    "    'datetime_features': len(datetime_cols),\n",
    "    'completeness_pct': float(((1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100)),\n",
    "    'duplicate_pct': float((df.duplicated().sum() / len(df) * 100)),\n",
    "}\n",
    "\n",
    "if TARGET_COL in df.columns:\n",
    "    summary['target_column'] = TARGET_COL\n",
    "    summary['target_distribution'] = df[TARGET_COL].value_counts().to_dict()\n",
    "\n",
    "# Salvar como JSON\n",
    "import json\n",
    "output_path = 'reports/eda_summary.json'\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nEDA Summary salvo em: {output_path}\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Document Control\n",
    "\n",
    "**EDA Status**: [Draft / In Review / Approved]\n",
    "\n",
    "**Reviewers**:\n",
    "- [ ] Data Scientist\n",
    "- [ ] Business Stakeholder\n",
    "- [ ] Data Engineer\n",
    "\n",
    "**Approval**: _________________________  Date: __________\n",
    "\n",
    "**Next Review**: [YYYY-MM-DD]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
